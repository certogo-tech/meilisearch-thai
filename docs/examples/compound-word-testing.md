# Thai Compound Word Testing Guide

This guide demonstrates how to test complex Thai compound words, including Thai-Japanese terms like "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞" (wakame seaweed).

## üéØ Understanding Complex Compound Words

### Thai-Japanese Food Terms

Thai cuisine incorporates many Japanese ingredients, creating compound words that combine Thai and Japanese elements:

- **‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞** (s«éa-l√†ai waa-gaa-m√©) = "wakame seaweed"
  - **‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢** (s«éa-l√†ai) = Thai word for "seaweed/algae"
  - **‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞** (waa-gaa-m√©) = Japanese "wakame" written in Thai script

### Why This is Challenging

1. **Mixed Language Origins**: Thai + Japanese transliteration
2. **Compound Structure**: Two distinct concepts joined together
3. **Search Expectations**: Users might search for either part
4. **Contextual Usage**: Used in food, health, and cooking contexts

## üß™ Testing "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞"

### Basic Tokenization Test

```bash
# Test basic tokenization
curl -X POST http://localhost:8001/api/v1/tokenize \
  -H "Content-Type: application/json" \
  -d '{"text": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"}'

# Expected result: Should properly separate the compound word
# ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "‡πÄ‡∏õ‡πá‡∏ô", "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•", "‡∏ó‡∏µ‡πà", "‡∏°‡∏µ", "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"]
```

### Search Query Processing Test

```bash
# Test query processing for compound word
curl -X POST http://localhost:8001/api/v1/query/process \
  -H "Content-Type: application/json" \
  -d '{
    "query": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
    "options": {
      "expand_compounds": true,
      "enable_partial_matching": true
    }
  }'

# Should generate search variants including:
# - "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞" (exact match)
# - "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢" (Thai part)
# - "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞" (Japanese part)
# - "*‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢*" (partial match)
# - "*‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞*" (partial match)
```

### Partial Search Tests

```bash
# Test searching for Thai part only
curl -X POST http://localhost:8001/api/v1/query/process \
  -H "Content-Type: application/json" \
  -d '{"query": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢", "options": {"expand_compounds": true}}'

# Test searching for Japanese part only
curl -X POST http://localhost:8001/api/v1/query/process \
  -H "Content-Type: application/json" \
  -d '{"query": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "options": {"expand_compounds": true}}'
```

## üìä Sample Test Data

### Test Documents

```json
{
  "test_documents": [
    {
      "id": "food_001",
      "title": "‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏•‡∏±‡∏î‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
      "content": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ó‡∏∞‡πÄ‡∏•‡∏à‡∏≤‡∏Å‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡∏´‡∏ß‡∏≤‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏™‡∏•‡∏±‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏ã‡∏∏‡∏õ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á",
      "category": "recipes",
      "tags": ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢", "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô", "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•", "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û"]
    },
    {
      "id": "health_001", 
      "title": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
      "content": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏≠‡∏∏‡∏î‡∏°‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÑ‡∏≠‡πÇ‡∏≠‡∏î‡∏µ‡∏ô ‡πÅ‡∏Ñ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢‡∏° ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô‡πÉ‡∏¢‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏ä‡πà‡∏ß‡∏¢‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏ï‡πà‡∏≠‡∏°‡πÑ‡∏ó‡∏£‡∏≠‡∏¢‡∏î‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡πà‡∏≠‡∏¢‡∏≠‡∏≤‡∏´‡∏≤‡∏£",
      "category": "health",
      "tags": ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢", "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û", "‡∏ß‡∏¥‡∏ï‡∏≤‡∏°‡∏¥‡∏ô", "‡πÅ‡∏£‡πà‡∏ò‡∏≤‡∏ï‡∏∏"]
    },
    {
      "id": "cooking_001",
      "title": "‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÅ‡∏´‡πâ‡∏á",
      "content": "‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÅ‡∏´‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏ô‡∏∏‡πà‡∏° ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ä‡πà‡∏ô‡πâ‡∏≥‡∏≠‡∏∏‡πà‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 10-15 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏à‡∏ô‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏•‡∏∞‡∏ô‡∏∏‡πà‡∏°",
      "category": "cooking",
      "tags": ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢", "‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ"]
    },
    {
      "id": "nutrition_001",
      "title": "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡πÇ‡∏†‡∏ä‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ä‡∏ô‡∏¥‡∏î‡∏ï‡πà‡∏≤‡∏á‡πÜ",
      "content": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏°‡∏µ‡πÑ‡∏≠‡πÇ‡∏≠‡∏î‡∏µ‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡πÇ‡∏ô‡∏£‡∏¥ ‡πÅ‡∏ï‡πà‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏Ñ‡∏≠‡∏°‡∏ö‡∏∏‡∏°‡∏µ‡πÇ‡∏õ‡∏£‡∏ï‡∏µ‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏ô‡∏¥‡∏î‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô",
      "category": "nutrition",
      "tags": ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢", "‡πÇ‡∏†‡∏ä‡∏ô‡∏≤‡∏Å‡∏≤‡∏£", "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö"]
    }
  ]
}
```

### Expected Search Results

```json
{
  "search_test_cases": [
    {
      "query": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
      "expected_documents": ["food_001", "health_001", "cooking_001", "nutrition_001"],
      "description": "Exact compound word should find all documents"
    },
    {
      "query": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢",
      "expected_documents": ["food_001", "health_001", "cooking_001", "nutrition_001"],
      "description": "Thai part should find all documents containing the compound"
    },
    {
      "query": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
      "expected_documents": ["food_001", "health_001", "cooking_001", "nutrition_001"],
      "description": "Japanese part should find all documents containing the compound"
    },
    {
      "query": "‡∏™‡∏•‡∏±‡∏î‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢",
      "expected_documents": ["food_001"],
      "description": "Context-specific search should find relevant documents"
    },
    {
      "query": "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•",
      "expected_documents": ["food_001"],
      "description": "Related terms should find contextually relevant documents"
    }
  ]
}
```

## üîß Advanced Testing Scenarios

### Mixed Content Testing

```bash
# Test document with mixed Thai-Japanese-English content
curl -X POST http://localhost:8001/api/v1/tokenize \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Wakame ‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ is a type of seaweed ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û"
  }'

# Should handle:
# - English: "Wakame", "is", "a", "type", "of", "seaweed"
# - Thai-Japanese compound: "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞"
# - Thai text: "‡∏ó‡∏µ‡πà", "‡∏°‡∏µ", "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå", "‡∏ï‡πà‡∏≠", "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û"
```

### Context-Aware Search Testing

```bash
# Test context-aware search
curl -X POST http://localhost:8001/api/v1/query/process \
  -H "Content-Type: application/json" \
  -d '{
    "query": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ ‡∏™‡∏π‡∏ï‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£",
    "options": {
      "expand_compounds": true,
      "context_aware": true
    }
  }'

# Should understand this is a food/recipe context
# and boost food-related documents
```

### Synonym and Related Terms Testing

```bash
# Test with synonyms and related terms
curl -X POST http://localhost:8001/api/v1/query/process \
  -H "Content-Type: application/json" \
  -d '{
    "query": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
    "options": {
      "expand_compounds": true,
      "include_synonyms": true,
      "related_terms": ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ó‡∏∞‡πÄ‡∏•", "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô", "wakame"]
    }
  }'
```

## üìà Performance Testing

### Tokenization Speed Test

```python
import time
import requests

def test_compound_word_performance():
    """Test tokenization performance with compound words"""
    
    test_texts = [
        "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",  # Simple compound
        "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå",  # Compound in sentence
        "‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏•‡∏±‡∏î‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÅ‡∏ö‡∏ö‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÅ‡∏ó‡πâ",  # Multiple compounds
        "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ ‡πÇ‡∏ô‡∏£‡∏¥ ‡∏Ñ‡∏≠‡∏°‡∏ö‡∏∏ ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ó‡∏∞‡πÄ‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ"  # Multiple seaweed types
    ]
    
    results = []
    for text in test_texts:
        times = []
        for _ in range(10):  # Run 10 times for average
            start = time.time()
            response = requests.post(
                "http://localhost:8001/api/v1/tokenize",
                json={"text": text}
            )
            end = time.time()
            
            if response.status_code == 200:
                times.append(end - start)
        
        avg_time = sum(times) / len(times) * 1000  # Convert to ms
        results.append({
            "text": text,
            "length": len(text),
            "avg_time_ms": avg_time,
            "chars_per_second": len(text) / (avg_time / 1000)
        })
    
    return results

# Run the test
performance_results = test_compound_word_performance()
for result in performance_results:
    print(f"Text: {result['text'][:30]}...")
    print(f"Speed: {result['chars_per_second']:.0f} chars/sec")
    print(f"Time: {result['avg_time_ms']:.2f}ms")
    print()
```

## üéØ Quality Validation

### Accuracy Testing Script

```python
def test_compound_word_accuracy():
    """Test accuracy of compound word tokenization"""
    
    test_cases = [
        {
            "input": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•",
            "expected_tokens": ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "‡πÄ‡∏õ‡πá‡∏ô", "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•"],
            "description": "Basic compound word recognition"
        },
        {
            "input": "‡∏ã‡∏∑‡πâ‡∏≠‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÅ‡∏´‡πâ‡∏á‡∏à‡∏≤‡∏Å‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô",
            "expected_contains": ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "‡πÅ‡∏´‡πâ‡∏á", "‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô"],
            "description": "Compound word in shopping context"
        },
        {
            "input": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡πÇ‡∏ô‡∏£‡∏¥",
            "expected_contains": ["‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "‡πÅ‡∏•‡∏∞", "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡πÇ‡∏ô‡∏£‡∏¥"],
            "description": "Multiple compound words"
        }
    ]
    
    results = []
    for case in test_cases:
        response = requests.post(
            "http://localhost:8001/api/v1/tokenize",
            json={"text": case["input"]}
        )
        
        if response.status_code == 200:
            tokens = response.json()["tokens"]
            
            if "expected_tokens" in case:
                accuracy = len(set(tokens) & set(case["expected_tokens"])) / len(case["expected_tokens"])
            else:
                accuracy = len(set(tokens) & set(case["expected_contains"])) / len(case["expected_contains"])
            
            results.append({
                "description": case["description"],
                "input": case["input"],
                "tokens": tokens,
                "accuracy": accuracy,
                "passed": accuracy >= 0.8
            })
    
    return results
```

## üîç Search Quality Testing

### Search Relevance Test

```python
def test_search_relevance():
    """Test search relevance for compound words"""
    
    # First, index test documents
    test_docs = [
        {
            "id": "1",
            "title": "‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏•‡∏±‡∏î‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
            "content": "‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏™‡∏•‡∏±‡∏î‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÅ‡∏ö‡∏ö‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô"
        },
        {
            "id": "2", 
            "title": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢",
            "content": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏°‡∏µ‡πÑ‡∏≠‡πÇ‡∏≠‡∏î‡∏µ‡∏ô‡∏™‡∏π‡∏á"
        },
        {
            "id": "3",
            "title": "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô",
            "content": "‡∏ã‡∏π‡∏ä‡∏¥ ‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥ ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≤‡∏á‡πÜ"
        }
    ]
    
    # Index documents (pseudo-code - actual implementation depends on your setup)
    # index_documents(test_docs)
    
    # Test search queries
    search_tests = [
        {
            "query": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
            "expected_top_results": ["1", "2"],  # Should rank recipe and health docs higher
            "description": "Exact compound word search"
        },
        {
            "query": "‡∏™‡∏•‡∏±‡∏î‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢",
            "expected_top_results": ["1"],  # Recipe should be top result
            "description": "Context-specific search"
        },
        {
            "query": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
            "expected_top_results": ["1", "2"],  # Should find compound word documents
            "description": "Japanese part search"
        }
    ]
    
    return search_tests
```

## üìö Integration with Demo Script

Let me update the demo script to include "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞" examples:

```python
# Add to demo_thai_tokenizer.py
def demo_compound_word_challenges():
    """Demo challenging compound words like Thai-Japanese terms"""
    print("üåä DEMO: Thai-Japanese Compound Words")
    print("=" * 50)
    
    challenging_compounds = [
        {
            "word": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
            "meaning": "wakame seaweed (Thai + Japanese)",
            "context": "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•"
        },
        {
            "word": "‡∏£‡∏≤‡πÄ‡∏°‡∏ô‡∏ï‡πâ‡∏ô‡∏Å‡∏≥‡πÄ‡∏ô‡∏¥‡∏î", 
            "meaning": "original ramen (Thai + Japanese + Thai)",
            "context": "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô"
        },
        {
            "word": "‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥‡∏õ‡∏•‡∏≤‡πÅ‡∏ã‡∏•‡∏°‡∏≠‡∏ô",
            "meaning": "salmon sashimi (Japanese + Thai + English)",
            "context": "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏î‡∏¥‡∏ö"
        }
    ]
    
    for item in challenging_compounds:
        print(f"Word: {item['word']}")
        print(f"Meaning: {item['meaning']}")
        print(f"Context: {item['context']}")
        
        # Test tokenization
        response = requests.post(
            f"{BASE_URL}/api/v1/tokenize",
            json={"text": f"{item['word']}‡πÄ‡∏õ‡πá‡∏ô{item['context']}‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"Tokens: {' | '.join(data['tokens'])}")
            print(f"Processing time: {data['processing_time_ms']}ms")
        
        print()
```

## üß™ Comprehensive Test Suite
