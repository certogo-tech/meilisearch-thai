#!/usr/bin/env python3
"""
Example: Using compound dictionary for improved Thai tokenization.
This example shows how to solve the wakame splitting issue.
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from tokenizer.factory import TokenizerFactory


def main():
    """Demonstrate compound dictionary usage."""
    
    print("üß™ Thai Compound Tokenization Example")
    print("=" * 40)
    
    # Test texts with compound words
    test_texts = [
        "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•",
        "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
        "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏°‡∏õ‡∏∏‡∏£‡∏∞",
        "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç",
        "‡πÇ‡∏ï‡πÇ‡∏¢‡∏ï‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô"
    ]
    
    print("\n=== Standard Tokenization (without compounds) ===")
    standard_segmenter = TokenizerFactory.create_segmenter(use_compounds=False)
    
    for text in test_texts:
        result = standard_segmenter.segment_text(text)
        print(f"Text: {text}")
        print(f"Tokens: {result.tokens}")
        print()
    
    print("\n=== Compound-Aware Tokenization ===")
    compound_segmenter = TokenizerFactory.create_wakame_optimized_segmenter()
    
    for text in test_texts:
        result = compound_segmenter.segment_text(text)
        print(f"Text: {text}")
        print(f"Tokens: {result.tokens}")
        
        # Check for specific compounds
        compounds_found = []
        target_compounds = ["‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥", "‡πÄ‡∏ó‡∏°‡∏õ‡∏∏‡∏£‡∏∞", "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï", "‡πÇ‡∏ï‡πÇ‡∏¢‡∏ï‡πâ‡∏≤", "‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤"]
        for compound in target_compounds:
            if compound in result.tokens:
                compounds_found.append(compound)
        
        if compounds_found:
            print(f"‚úÖ Compounds preserved: {compounds_found}")
        print()
    
    print("\n=== Dictionary Statistics ===")
    stats = TokenizerFactory.get_dictionary_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")


if __name__ == "__main__":
    main()
