#!/usr/bin/env python3
"""
Example: Using compound dictionary for improved Thai tokenization.
This example shows how to solve the wakame splitting issue.
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from tokenizer.factory import TokenizerFactory


def main():
    """Demonstrate compound dictionary usage."""
    
    print("üß™ Thai Compound Tokenization Example")
    print("=" * 40)
    
    # Test texts with compound words (including new ones)
    test_texts = [
        "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•",
        "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û", 
        "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏°‡∏õ‡∏∏‡∏£‡∏∞",
        "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç",
        "‡πÇ‡∏ï‡πÇ‡∏¢‡∏ï‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô",
        # New test cases with our added compounds
        "‡∏â‡∏±‡∏ô‡∏ä‡∏≠‡∏ö‡∏Å‡∏¥‡∏ô‡∏û‡∏¥‡∏ã‡∏ã‡πà‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏ú‡∏±‡∏î‡∏ã‡∏µ‡∏≠‡∏¥‡πä‡∏ß",
        "‡∏Ç‡πâ‡∏≤‡∏ß‡∏ú‡∏±‡∏î‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏™‡∏•‡∏±‡∏î‡∏ú‡∏•‡πÑ‡∏°‡πâ‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å", 
        "‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡πÅ‡∏ü‡πÄ‡∏¢‡πá‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏≤‡πÄ‡∏¢‡πá‡∏ô‡∏°‡∏≤‡∏î‡∏∑‡πà‡∏°",
        "‡∏™‡∏°‡∏≤‡∏£‡πå‡∏ó‡πÇ‡∏ü‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ",
        "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏°‡∏¥‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢",
        "‡∏Ñ‡∏•‡∏≤‡∏ß‡∏î‡πå‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏Å‡∏Ñ‡∏¥‡∏ß‡∏£‡∏¥‡∏ï‡∏µ‡πâ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏¢‡∏∏‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡∏î‡∏≤‡∏ï‡πâ‡∏≤"
    ]
    
    print("\n=== Standard Tokenization (without compounds) ===")
    standard_segmenter = TokenizerFactory.create_segmenter(use_compounds=False)
    
    for text in test_texts:
        result = standard_segmenter.segment_text(text)
        print(f"Text: {text}")
        print(f"Tokens: {result.tokens}")
        print()
    
    print("\n=== Compound-Aware Tokenization ===")
    compound_segmenter = TokenizerFactory.create_wakame_optimized_segmenter()
    
    for text in test_texts:
        result = compound_segmenter.segment_text(text)
        print(f"Text: {text}")
        print(f"Tokens: {result.tokens}")
        
        # Check for specific compounds (including new ones)
        compounds_found = []
        target_compounds = [
            "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥", "‡πÄ‡∏ó‡∏°‡∏õ‡∏∏‡∏£‡∏∞", "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï", "‡πÇ‡∏ï‡πÇ‡∏¢‡∏ï‡πâ‡∏≤", "‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤",
            "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "‡∏û‡∏¥‡∏ã‡∏ã‡πà‡∏≤‡πÑ‡∏ó‡∏¢", "‡πÄ‡∏™‡πâ‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏ú‡∏±‡∏î‡∏ã‡∏µ‡∏≠‡∏¥‡πä‡∏ß", "‡∏Ç‡πâ‡∏≤‡∏ß‡∏ú‡∏±‡∏î‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏±‡∏ô", "‡∏™‡∏•‡∏±‡∏î‡∏ú‡∏•‡πÑ‡∏°‡πâ",
            "‡∏Å‡∏≤‡πÅ‡∏ü‡πÄ‡∏¢‡πá‡∏ô", "‡∏ä‡∏≤‡πÄ‡∏¢‡πá‡∏ô", "‡∏™‡∏°‡∏≤‡∏£‡πå‡∏ó‡πÇ‡∏ü‡∏ô", "‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô", "‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏°‡∏¥‡πà‡∏á", "‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå",
            "‡∏Ñ‡∏•‡∏≤‡∏ß‡∏î‡πå‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå", "‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏Å‡∏Ñ‡∏¥‡∏ß‡∏£‡∏¥‡∏ï‡∏µ‡πâ", "‡∏ö‡∏¥‡πä‡∏Å‡∏î‡∏≤‡∏ï‡πâ‡∏≤"
        ]
        for compound in target_compounds:
            if compound in result.tokens:
                compounds_found.append(compound)
        
        if compounds_found:
            print(f"‚úÖ Compounds preserved: {compounds_found}")
        print()
    
    print("\n=== Dictionary Statistics ===")
    stats = TokenizerFactory.get_dictionary_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")


if __name__ == "__main__":
    main()
