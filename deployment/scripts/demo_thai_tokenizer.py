#!/usr/bin/env python3
"""
Thai Tokenizer Demonstration Script

This script demonstrates practical usage scenarios for the Thai Tokenizer API,
showing real-world examples of Thai text processing, compound word handling,
and search query enhancement.

Usage:
    python3 deployment/scripts/demo_thai_tokenizer.py

Requirements:
    - Thai Tokenizer service running on localhost:8001
    - requests library installed
"""

import requests
import json

BASE_URL = "http://localhost:8001"

def demo_basic_tokenization():
    """Demo basic Thai tokenization"""
    print("üî§ DEMO 1: Basic Thai Tokenization")
    print("=" * 40)
    
    texts = [
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö",
        "‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå",
        "‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á",
        "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®"
    ]
    
    for text in texts:
        response = requests.post(
            f"{BASE_URL}/api/v1/tokenize",
            json={"text": text}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"Input:  {text}")
            print(f"Output: {' | '.join(data['tokens'])}")
            print(f"Time:   {data['processing_time_ms']}ms")
            print()

def demo_compound_words():
    """Demo compound word handling"""
    print("üîó DEMO 2: Compound Word Handling")
    print("=" * 40)
    
    compound_examples = [
        "‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ò‡∏¥‡∏Å‡∏≤‡∏£",
        "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô",
        "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£",
        "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£"
    ]
    
    for text in compound_examples:
        response = requests.post(
            f"{BASE_URL}/api/v1/tokenize",
            json={"text": text}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"Compound: {text}")
            print(f"Tokens:   {' + '.join(data['tokens'])}")
            print(f"Count:    {len(data['tokens'])} tokens")
            print()

def demo_mixed_content():
    """Demo mixed Thai-English content"""
    print("üåê DEMO 3: Mixed Thai-English Content")
    print("=" * 40)
    
    mixed_texts = [
        "‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ AI ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢",
        "Machine Learning ‡πÅ‡∏•‡∏∞ Deep Learning",
        "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó Google ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢",
        "COVID-19 ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢"
    ]
    
    for text in mixed_texts:
        response = requests.post(
            f"{BASE_URL}/api/v1/tokenize",
            json={"text": text}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"Mixed:  {text}")
            print(f"Tokens: {data['tokens']}")
            print()

def demo_search_query_processing():
    """Demo search query processing"""
    print("üîç DEMO 4: Search Query Processing")
    print("=" * 40)
    
    queries = [
        "‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå",
        "‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ",
        "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ AI"
    ]
    
    for query in queries:
        response = requests.post(
            f"{BASE_URL}/api/v1/query/process",
            json={
                "query": query,
                "options": {"expand_compounds": True}
            }
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"Query: {query}")
            print(f"Processed: {data['processed_query']}")
            print(f"Variants: {data['search_variants'][:3]}...")  # Show first 3
            print(f"Tokens: {len(data['query_tokens'])}")
            print()

def demo_performance_comparison():
    """Demo performance with different text lengths"""
    print("‚ö° DEMO 5: Performance Comparison")
    print("=" * 40)
    
    texts = {
        "Short": "‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå",
        "Medium": "‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢",
        "Long": "‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏´‡∏°‡πà‡πÜ ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô"
    }
    
    for length, text in texts.items():
        response = requests.post(
            f"{BASE_URL}/api/v1/tokenize",
            json={"text": text}
        )
        
        if response.status_code == 200:
            data = response.json()
            char_count = len(text)
            token_count = len(data['tokens'])
            processing_time = data['processing_time_ms']
            speed = char_count / (processing_time / 1000) if processing_time > 0 else 0
            
            print(f"{length} Text:")
            print(f"  Characters: {char_count}")
            print(f"  Tokens: {token_count}")
            print(f"  Time: {processing_time}ms")
            print(f"  Speed: {speed:.0f} chars/sec")
            print()

def demo_thai_japanese_compounds():
    """Demo Thai-Japanese compound words like wakame seaweed"""
    print("üåä DEMO 6: Thai-Japanese Compound Words")
    print("=" * 40)
    
    thai_japanese_examples = {
        "Wakame Seaweed": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå",
        "Ramen Noodles": "‡∏£‡∏≤‡πÄ‡∏°‡∏ô‡∏ï‡πâ‡∏ô‡∏ï‡∏≥‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô", 
        "Sashimi Fish": "‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥‡∏õ‡∏•‡∏≤‡πÅ‡∏ã‡∏•‡∏°‡∏≠‡∏ô‡∏™‡∏î‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏ï‡∏•‡∏≤‡∏î‡∏õ‡∏•‡∏≤",
        "Miso Soup": "‡∏ã‡∏∏‡∏õ‡∏°‡∏¥‡πÇ‡∏ã‡∏∞‡∏£‡πâ‡∏≠‡∏ô‡πÜ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏´‡∏ô‡∏≤‡∏ß",
        "Tempura Style": "‡πÄ‡∏ó‡∏°‡∏õ‡∏∏‡∏£‡∏∞‡∏ú‡∏±‡∏Å‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÅ‡∏ó‡πâ"
    }
    
    for category, text in thai_japanese_examples.items():
        response = requests.post(
            f"{BASE_URL}/api/v1/tokenize",
            json={"text": text}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"{category}:")
            print(f"  Text: {text}")
            print(f"  Tokens: {' | '.join(data['tokens'])}")
            print(f"  Count: {len(data['tokens'])} tokens")
            print()

def demo_real_world_examples():
    """Demo with real-world Thai text examples"""
    print("üåç DEMO 7: Real-World Examples")
    print("=" * 40)
    
    examples = {
        "News Headline": "‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÄ‡∏£‡πà‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÑ‡∏ó‡∏¢",
        "Academic Text": "‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÉ‡∏ô‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÑ‡∏ó‡∏¢",
        "Business": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏à‡∏≥‡∏Å‡∏±‡∏î",
        "Government": "‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°",
        "Food Review": "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏™‡∏î‡πÉ‡∏´‡∏°‡πà"
    }
    
    for category, text in examples.items():
        response = requests.post(
            f"{BASE_URL}/api/v1/tokenize",
            json={"text": text}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"{category}:")
            print(f"  Text: {text}")
            print(f"  Tokens: {' | '.join(data['tokens'])}")
            print(f"  Count: {len(data['tokens'])} tokens")
            print()

def main():
    """Run all demos"""
    print("üöÄ Thai Tokenizer Practical Demos")
    print("=" * 50)
    
    # Check if service is running
    try:
        response = requests.get(f"{BASE_URL}/health")
        if response.status_code != 200:
            print("‚ùå Service not available. Please start the Thai tokenizer first.")
            return
    except requests.exceptions.ConnectionError:
        print("‚ùå Cannot connect to Thai tokenizer. Please start the service first.")
        return
    
    print("‚úÖ Service is running. Starting demos...\n")
    
    # Run all demos
    demo_basic_tokenization()
    demo_compound_words()
    demo_mixed_content()
    demo_search_query_processing()
    demo_performance_comparison()
    demo_thai_japanese_compounds()
    demo_real_world_examples()
    
    print("üéâ All demos completed!")
    print("\nüí° Tips:")
    print("- Visit http://localhost:8001/docs for full API documentation")
    print("- Use the tokenizer for Thai search applications")
    print("- Integrate with MeiliSearch for full-text search")
    print("- Monitor performance with the health endpoint")

if __name__ == "__main__":
    main()