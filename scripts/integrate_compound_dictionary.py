#!/usr/bin/env python3
"""
Integration script to demonstrate using the compound words dictionary
with the existing Thai tokenizer system.
"""

import json
import sys
import os
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from tokenizer.config_manager import ConfigManager, ThaiTokenizerSettings
    from tokenizer.thai_segmenter import ThaiSegmenter
    CONFIG_AVAILABLE = True
except ImportError as e:
    print(f"Config system not available: {e}")
    CONFIG_AVAILABLE = False

# Fallback to direct PyThaiNLP usage
try:
    from pythainlp import word_tokenize
    from pythainlp.tokenize import Tokenizer
    from pythainlp.corpus.common import thai_words
    PYTHAINLP_AVAILABLE = True
except ImportError:
    PYTHAINLP_AVAILABLE = False
    print("PyThaiNLP not available. Please install with: pip install pythainlp")


def load_compound_dictionary(dict_path: str) -> list:
    """Load compound words from JSON dictionary file."""
    try:
        with open(dict_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Handle different JSON formats
        compounds = []
        if isinstance(data, list):
            compounds = data
        elif isinstance(data, dict):
            for category, words in data.items():
                if isinstance(words, list):
                    compounds.extend(words)
        
        print(f"‚úÖ Loaded {len(compounds)} compound words from dictionary")
        return compounds
    
    except Exception as e:
        print(f"‚ùå Failed to load dictionary: {e}")
        return []


def test_with_config_manager():
    """Test using the ConfigManager approach."""
    if not CONFIG_AVAILABLE:
        print("‚ö†Ô∏è  ConfigManager not available, skipping this test")
        return
    
    print("\n=== Testing with ConfigManager Integration ===")
    
    try:
        # Create config with custom dictionary path
        config_file = Path(__file__).parent.parent / "config" / "development" / "tokenizer.json"
        config_manager = ConfigManager(config_file=config_file)
        
        # Get tokenizer config
        tokenizer_config = config_manager.get_tokenizer_config()
        print(f"‚úÖ Custom dictionary size: {len(tokenizer_config.custom_dictionary)}")
        
        # Create segmenter with custom dictionary
        segmenter = ThaiSegmenter(
            engine=tokenizer_config.engine.value,
            custom_dict=tokenizer_config.custom_dictionary,
            keep_whitespace=tokenizer_config.keep_whitespace
        )
        
        # Test wakame tokenization
        test_texts = [
            "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
            "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå",
            "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞"
        ]
        
        for text in test_texts:
            result = segmenter.segment_text(text)
            wakame_found = '‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞' in result.tokens
            print(f"Text: {text}")
            print(f"Tokens: {result.tokens}")
            print(f"‚úÖ ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ as single token: {wakame_found}")
            print()
            
    except Exception as e:
        print(f"‚ùå ConfigManager test failed: {e}")


def test_direct_integration():
    """Test direct integration with PyThaiNLP."""
    if not PYTHAINLP_AVAILABLE:
        print("‚ö†Ô∏è  PyThaiNLP not available, skipping this test")
        return
    
    print("\n=== Testing Direct PyThaiNLP Integration ===")
    
    # Load compound dictionary
    dict_path = Path(__file__).parent.parent / "data" / "dictionaries" / "thai_compounds.json"
    custom_words = load_compound_dictionary(str(dict_path))
    
    if not custom_words:
        print("‚ùå No custom words loaded, cannot proceed")
        return
    
    try:
        # Create custom tokenizer
        custom_word_set = set(thai_words()) | set(custom_words)
        custom_tokenizer = Tokenizer(custom_word_set)
        
        # Test cases
        test_cases = [
            {
                "text": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
                "expected_compound": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
                "description": "Basic wakame compound"
            },
            {
                "text": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
                "expected_compound": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
                "description": "Wakame in sentence"
            },
            {
                "text": "‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏°‡∏õ‡∏∏‡∏£‡∏∞",
                "expected_compound": "‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥",
                "description": "Multiple Japanese compounds"
            },
            {
                "text": "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï",
                "expected_compound": "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå",
                "description": "English compounds"
            }
        ]
        
        success_count = 0
        for test_case in test_cases:
            tokens = custom_tokenizer.word_tokenize(test_case["text"])
            compound_found = test_case["expected_compound"] in tokens
            
            print(f"Test: {test_case['description']}")
            print(f"Text: {test_case['text']}")
            print(f"Tokens: {tokens}")
            print(f"‚úÖ {test_case['expected_compound']} found: {compound_found}")
            
            if compound_found:
                success_count += 1
            print()
        
        print(f"üéØ Success rate: {success_count}/{len(test_cases)} ({success_count/len(test_cases)*100:.1f}%)")
        
    except Exception as e:
        print(f"‚ùå Direct integration test failed: {e}")


def create_environment_config():
    """Create environment configuration for compound words."""
    print("\n=== Creating Environment Configuration ===")
    
    env_config = {
        "THAI_TOKENIZER_CUSTOM_DICTIONARY_PATH": "data/dictionaries/thai_compounds.json",
        "THAI_TOKENIZER_TOKENIZER_HANDLE_COMPOUNDS": "true",
        "THAI_TOKENIZER_TOKENIZER_ENGINE": "newmm",
        "THAI_TOKENIZER_DEBUG": "true"
    }
    
    env_file_path = Path(__file__).parent.parent / ".env.compound"
    
    try:
        with open(env_file_path, 'w', encoding='utf-8') as f:
            for key, value in env_config.items():
                f.write(f"{key}={value}\n")
        
        print(f"‚úÖ Created environment config: {env_file_path}")
        print("To use this config, run: export $(cat .env.compound | xargs)")
        
    except Exception as e:
        print(f"‚ùå Failed to create environment config: {e}")


def main():
    """Main integration demonstration."""
    print("üöÄ Thai Compound Words Dictionary Integration")
    print("=" * 50)
    
    # Test direct integration first (most reliable)
    test_direct_integration()
    
    # Test with config manager
    test_with_config_manager()
    
    # Create environment configuration
    create_environment_config()
    
    print("\nüìã Integration Summary:")
    print("1. ‚úÖ Compound dictionary created at data/dictionaries/thai_compounds.json")
    print("2. ‚úÖ Configuration files created in config/development/")
    print("3. ‚úÖ Environment variables template created as .env.compound")
    print("4. ‚úÖ ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ tokenization issue resolved with custom dictionary")
    
    print("\nüîß Next Steps:")
    print("1. Update your tokenizer initialization to use custom_dict parameter")
    print("2. Set THAI_TOKENIZER_CUSTOM_DICTIONARY_PATH environment variable")
    print("3. Restart your tokenizer service to load the new dictionary")
    print("4. Test with your existing wakame test cases")


if __name__ == "__main__":
    main()