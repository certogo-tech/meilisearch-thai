#!/usr/bin/env python3
"""
Simple test to verify that adding ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ to custom dictionary solves the issue.
This is a standalone script that demonstrates the fix.
"""

import json
from pathlib import Path

try:
    from pythainlp import word_tokenize
    from pythainlp.tokenize import Tokenizer
    from pythainlp.corpus.common import thai_words
    PYTHAINLP_AVAILABLE = True
except ImportError:
    print("‚ùå PyThaiNLP not available. Please install with: pip install pythainlp")
    PYTHAINLP_AVAILABLE = False


def load_compound_dictionary():
    """Load compound words from the dictionary file."""
    dict_path = Path("data/dictionaries/thai_compounds.json")
    
    if not dict_path.exists():
        print(f"‚ùå Dictionary not found: {dict_path}")
        return []
    
    try:
        with open(dict_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        compounds = []
        for category, words in data.items():
            if isinstance(words, list):
                compounds.extend(words)
        
        return compounds
    except Exception as e:
        print(f"‚ùå Failed to load dictionary: {e}")
        return []


def test_wakame_fix():
    """Test that ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ is now tokenized correctly."""
    
    if not PYTHAINLP_AVAILABLE:
        return
    
    print("üß™ Testing Wakame Tokenization Fix")
    print("=" * 35)
    
    # Load compound dictionary
    compounds = load_compound_dictionary()
    print(f"üìö Loaded {len(compounds)} compound words")
    
    if "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞" not in compounds:
        print("‚ùå ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ not found in dictionary!")
        return
    else:
        print("‚úÖ ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ found in dictionary")
    
    # Create custom tokenizer
    try:
        custom_word_set = set(thai_words()) | set(compounds)
        custom_tokenizer = Tokenizer(custom_word_set)
        print("‚úÖ Custom tokenizer created")
    except Exception as e:
        print(f"‚ùå Failed to create tokenizer: {e}")
        return
    
    # Test cases from the original wakame test
    test_cases = [
        {
            "text": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
            "description": "Basic wakame seaweed compound"
        },
        {
            "text": "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∞‡πÄ‡∏•",
            "description": "Wakame in sentence context"
        },
        {
            "text": "‡∏™‡∏•‡∏±‡∏î‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÅ‡∏ö‡∏ö‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô",
            "description": "Wakame salad with context"
        },
        {
            "text": "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
            "description": "Restaurant serving wakame"
        },
        {
            "text": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
            "description": "Wakame health benefits"
        }
    ]
    
    print("\\nüîç Test Results:")
    print("-" * 50)
    
    success_count = 0
    for i, test_case in enumerate(test_cases, 1):
        tokens = custom_tokenizer.word_tokenize(test_case["text"])
        
        # Check if ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ appears as single token
        wakame_as_single = "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞" in tokens
        
        # Check if it's NOT split into ‡∏ß‡∏≤, ‡∏Å‡∏≤, ‡πÄ‡∏°‡∏∞
        not_split = not ("‡∏ß‡∏≤" in tokens and "‡∏Å‡∏≤" in tokens and "‡πÄ‡∏°‡∏∞" in tokens)
        
        success = wakame_as_single or not_split
        if success:
            success_count += 1
        
        status = "‚úÖ" if success else "‚ùå"
        print(f"{status} Test {i}: {test_case['description']}")
        print(f"   Text: {test_case['text']}")
        print(f"   Tokens: {tokens}")
        print(f"   ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ as single token: {wakame_as_single}")
        print()
    
    print("üìä Summary:")
    print(f"   Success rate: {success_count}/{len(test_cases)} ({success_count/len(test_cases)*100:.1f}%)")
    
    if success_count == len(test_cases):
        print("üéâ All tests passed! ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ tokenization issue is FIXED!")
    elif success_count > len(test_cases) // 2:
        print("‚úÖ Most tests passed. Significant improvement achieved!")
    else:
        print("‚ö†Ô∏è  Some tests still failing. May need additional dictionary entries.")
    
    # Test other compounds too
    print("\\nüåü Testing Other Compound Words:")
    other_compounds = ["‡∏ã‡∏≤‡∏ä‡∏¥‡∏°‡∏¥", "‡πÄ‡∏ó‡∏°‡∏õ‡∏∏‡∏£‡∏∞", "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï"]
    
    for compound in other_compounds:
        test_text = f"‡∏â‡∏±‡∏ô‡∏ä‡∏≠‡∏ö{compound}‡∏°‡∏≤‡∏Å"
        tokens = custom_tokenizer.word_tokenize(test_text)
        found = compound in tokens
        status = "‚úÖ" if found else "‚ùå"
        print(f"   {status} {compound}: {tokens}")


def main():
    """Main test function."""
    print("üöÄ Wakame Tokenization Fix Verification")
    print("=" * 40)
    print("This test verifies that adding ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ to the custom dictionary")
    print("solves the tokenization splitting issue.")
    print()
    
    test_wakame_fix()
    
    print("\\nüí° Integration Notes:")
    print("- Dictionary file: data/dictionaries/thai_compounds.json")
    print("- TokenizerFactory: src/tokenizer/factory.py")
    print("- Environment setup: scripts/setup_compound_env.sh")
    print("- Usage example: examples/compound_tokenization.py")


if __name__ == "__main__":
    main()