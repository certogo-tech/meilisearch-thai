# Manual Testing Guide for Researcher Index with ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞

This guide provides step-by-step manual commands to test ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ tokenization in your existing "researcher" index.

## üî¨ **Quick Test Commands**

### **1. Check Your Researcher Index**
```bash
# Check if researcher index exists and get document count
curl -X GET "http://10.0.2.105:7700/indexes/researcher" \
  -H "Authorization: Bearer FKjPKTmFnCl7EPg6YLula1DC6n5mHqId"
```

### **2. Search for ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ in Existing Data**
```bash
# Search for documents containing ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ in your researcher index
curl -X POST "http://10.0.2.105:7700/indexes/researcher/search" \
  -H "Authorization: Bearer FKjPKTmFnCl7EPg6YLula1DC6n5mHqId" \
  -H "Content-Type: application/json" \
  -d '{
    "q": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
    "limit": 10,
    "attributesToHighlight": ["*"]
  }'
```

**What to look for:**
- ‚úÖ `"hits": [...]` with results = ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ is searchable
- ‚ùå `"hits": []` empty = ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ not found or poorly tokenized

### **3. Test Current Tokenization Quality**
```bash
# Test how the current tokenizer handles ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞
curl -X POST "https://search.cads.arda.or.th/api/v1/tokenize/compound" \
  -H "Content-Type: application/json" \
  -d '{"text": "‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞"}'
```

**Expected good result:**
```json
{
  "tokens": ["‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢", "‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢", "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞"],
  "processing_time_ms": 0
}
```

**Bad result (if ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ splits):**
```json
{
  "tokens": ["‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢", "‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢", "‡∏ß‡∏≤", "‡∏Å‡∏≤", "‡πÄ‡∏°‡∏∞"]
}
```

### **4. Test New Document Indexing**
```bash
# Index a new test document with ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞
curl -X POST "https://search.cads.arda.or.th/api/v1/index-document" \
  -H "Content-Type: application/json" \
  -d '{
    "id": "wakame-researcher-test",
    "title": "‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
    "content": "‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û"
  }'
```

**Expected result:**
```json
{
  "document_id": "wakame-researcher-test",
  "status": "completed",
  "tokenized_fields": {
    "thai_content": {
      "tokens": ["‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢", "‡∏û‡∏ö", "‡∏ß‡πà‡∏≤", "‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢", "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "‡∏°‡∏µ", "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå", "‡∏ï‡πà‡∏≠", "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û"]
    }
  }
}
```

### **5. Search for Your New Document**
```bash
# Wait 2 seconds for indexing, then search
sleep 2

curl -X POST "http://10.0.2.105:7700/indexes/researcher/search" \
  -H "Authorization: Bearer FKjPKTmFnCl7EPg6YLula1DC6n5mHqId" \
  -H "Content-Type: application/json" \
  -d '{
    "q": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞",
    "limit": 10,
    "attributesToHighlight": ["title", "content"]
  }'
```

**What to look for:**
- Your new document should appear in results
- ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ should be highlighted in the content
- Should show improved tokenization

## üß™ **Automated Testing Script**

Run the comprehensive test:
```bash
./scripts/testing/test-researcher-index.sh
```

This script will:
- ‚úÖ Check your researcher index
- ‚úÖ Search for existing ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ documents
- ‚úÖ Test current tokenization quality
- ‚úÖ Index a new test document
- ‚úÖ Verify the new document is searchable
- ‚úÖ Compare old vs new tokenization

## üîç **Manual Verification Steps**

### **Step 1: Check Current Search Quality**
```bash
# Test various compound word searches in your researcher index
for term in "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞" "‡∏ã‡∏π‡∏ä‡∏¥" "‡πÄ‡∏ó‡∏°‡∏õ‡∏∏‡∏£‡∏∞" "‡∏£‡∏≤‡πÄ‡∏°‡∏ô"; do
  echo "Testing: $term"
  curl -s -X POST "http://10.0.2.105:7700/indexes/researcher/search" \
    -H "Authorization: Bearer FKjPKTmFnCl7EPg6YLula1DC6n5mHqId" \
    -d "{\"q\": \"$term\", \"limit\": 5}" | \
    grep -o '"hits":\[[^]]*\]' | grep -o '{"' | wc -l
done
```

### **Step 2: Test Specific Research Content**
```bash
# Test with research-specific content containing ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞
curl -X POST "https://search.cads.arda.or.th/api/v1/tokenize/compound" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏≤‡∏´‡∏£‡πà‡∏≤‡∏¢‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏†‡∏ä‡∏ô‡∏≤‡∏Å‡∏≤‡∏£"
  }'
```

### **Step 3: Compare Before/After**
```bash
# Before: Search existing documents
echo "=== EXISTING DOCUMENTS ==="
curl -s -X POST "http://10.0.2.105:7700/indexes/researcher/search" \
  -H "Authorization: Bearer FKjPKTmFnCl7EPg6YLula1DC6n5mHqId" \
  -d '{"q": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "limit": 5}' | \
  jq '.hits | length'

# After: Index new document and search
echo "=== AFTER NEW INDEXING ==="
curl -X POST "https://search.cads.arda.or.th/api/v1/index-document" \
  -H "Content-Type: application/json" \
  -d '{
    "id": "comparison-test",
    "content": "‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå"
  }' > /dev/null

sleep 2

curl -s -X POST "http://10.0.2.105:7700/indexes/researcher/search" \
  -H "Authorization: Bearer FKjPKTmFnCl7EPg6YLula1DC6n5mHqId" \
  -d '{"q": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞", "limit": 10}' | \
  jq '.hits | length'
```

## üìä **Success Criteria**

### **‚úÖ Good Tokenization:**
- ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ appears as single token: `["‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞"]`
- Search finds documents containing ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞
- New documents are properly indexed and searchable
- Processing time < 10ms

### **‚ùå Poor Tokenization:**
- ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ splits into: `["‡∏ß‡∏≤", "‡∏Å‡∏≤", "‡πÄ‡∏°‡∏∞"]`
- Search doesn't find ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ documents
- Inconsistent search results

## üîß **If Tokenization is Poor**

If your tests show poor tokenization:

1. **Check dictionary loading:**
   ```bash
   ./scripts/maintenance/debug-dictionary.sh
   ```

2. **Rebuild with dictionary fix:**
   ```bash
   ./scripts/deployment/rebuild-with-dictionary.sh
   ```

3. **Consider selective reindexing:**
   ```bash
   ./scripts/maintenance/assess-reindexing-need.sh
   ```

## üéØ **Quick Quality Check**

**One-liner to test everything:**
```bash
echo "Testing ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ tokenization..." && \
curl -s -X POST "https://search.cads.arda.or.th/api/v1/tokenize/compound" \
  -H "Content-Type: application/json" \
  -d '{"text": "‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞"}' | \
grep -q '"‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞"' && \
echo "‚úÖ ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ preserved as compound" || \
echo "‚ùå ‡∏ß‡∏≤‡∏Å‡∏≤‡πÄ‡∏°‡∏∞ not preserved"
```

This will quickly tell you if your compound word tokenization is working correctly!